name: Edge Function Performance Tests

on:
  push:
    branches: [main, develop]
    paths:
      - 'supabase/functions/**'
      - 'scripts/performance-test-edge-functions.ts'
  pull_request:
    branches: [main]
    paths:
      - 'supabase/functions/**'
      - 'scripts/performance-test-edge-functions.ts'
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_scenario:
        description: 'Test scenario to run'
        required: false
        default: 'light'
        type: choice
        options:
          - light
          - moderate
          - heavy
          - stress

jobs:
  edge-function-tests:
    name: Edge Function Performance Tests
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        scenario: [light, moderate]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Setup Supabase CLI
        uses: supabase/setup-cli@v1
        with:
          version: latest
      
      - name: Start Supabase
        run: supabase start
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
      
      - name: Deploy Edge Functions
        run: |
          supabase functions deploy document-processor \
            --project-ref ${{ secrets.SUPABASE_PROJECT_REF }}
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
      
      - name: Wait for deployment
        run: sleep 30
      
      - name: Run Edge Function Performance Tests
        run: |
          scenario="${{ matrix.scenario }}"
          if [ "${{ github.event.inputs.test_scenario }}" != "" ]; then
            scenario="${{ github.event.inputs.test_scenario }}"
          fi
          npm run test:edge-functions:$scenario
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      
      - name: Upload Test Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: edge-function-test-reports-${{ matrix.scenario }}
          path: test-reports/
          retention-days: 30
      
      - name: Parse Test Results
        id: test-results
        run: |
          # Extract key metrics from latest test report
          latest_report=$(ls -t test-reports/*.json | head -n1)
          
          if [ -f "$latest_report" ]; then
            success_rate=$(jq '.summary.successRate' "$latest_report")
            avg_response_time=$(jq '.summary.averageResponseTime' "$latest_report")
            p95_response_time=$(jq '.summary.p95ResponseTime' "$latest_report")
            requests_per_second=$(jq '.summary.requestsPerSecond' "$latest_report")
            
            echo "success_rate=$success_rate" >> $GITHUB_OUTPUT
            echo "avg_response_time=$avg_response_time" >> $GITHUB_OUTPUT
            echo "p95_response_time=$p95_response_time" >> $GITHUB_OUTPUT
            echo "requests_per_second=$requests_per_second" >> $GITHUB_OUTPUT
            
            # Determine if test passed performance thresholds
            if (( $(echo "$success_rate >= 95" | bc -l) )) && \
               (( $(echo "$p95_response_time <= 5000" | bc -l) )); then
              echo "performance_passed=true" >> $GITHUB_OUTPUT
            else
              echo "performance_passed=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "performance_passed=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Comment PR with Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const scenario = '${{ matrix.scenario }}';
            const successRate = '${{ steps.test-results.outputs.success_rate }}';
            const avgResponseTime = '${{ steps.test-results.outputs.avg_response_time }}';
            const p95ResponseTime = '${{ steps.test-results.outputs.p95_response_time }}';
            const requestsPerSecond = '${{ steps.test-results.outputs.requests_per_second }}';
            const performancePassed = '${{ steps.test-results.outputs.performance_passed }}';
            
            const emoji = performancePassed === 'true' ? '✅' : '❌';
            const status = performancePassed === 'true' ? 'PASSED' : 'FAILED';
            
            const comment = `## ${emoji} Edge Function Performance Test Results (${scenario})
            
            **Status:** ${status}
            
            ### Key Metrics
            - **Success Rate:** ${parseFloat(successRate).toFixed(2)}%
            - **Average Response Time:** ${parseFloat(avgResponseTime).toFixed(2)}ms
            - **95th Percentile Response Time:** ${parseFloat(p95ResponseTime).toFixed(2)}ms
            - **Requests per Second:** ${parseFloat(requestsPerSecond).toFixed(2)}
            
            ### Performance Thresholds
            - Success Rate: ${parseFloat(successRate).toFixed(2)}% (Required: ≥95%) ${parseFloat(successRate) >= 95 ? '✅' : '❌'}
            - P95 Response Time: ${parseFloat(p95ResponseTime).toFixed(2)}ms (Required: ≤5000ms) ${parseFloat(p95ResponseTime) <= 5000 ? '✅' : '❌'}
            
            <details>
            <summary>Test Configuration</summary>
            
            - **Scenario:** ${scenario}
            - **Test Duration:** Varies by scenario
            - **Concurrent Users:** Varies by scenario
            </details>
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
      
      - name: Fail if Performance Thresholds Not Met
        if: steps.test-results.outputs.performance_passed == 'false'
        run: |
          echo "❌ Performance tests failed to meet thresholds"
          echo "Success Rate: ${{ steps.test-results.outputs.success_rate }}% (Required: ≥95%)"
          echo "P95 Response Time: ${{ steps.test-results.outputs.p95_response_time }}ms (Required: ≤5000ms)"
          exit 1

  notification:
    name: Send Performance Report
    runs-on: ubuntu-latest
    needs: edge-function-tests
    if: always() && github.event_name == 'schedule'
    
    steps:
      - name: Download Test Reports
        uses: actions/download-artifact@v4
        with:
          pattern: edge-function-test-reports-*
          merge-multiple: true
          path: test-reports/
      
      - name: Generate Summary Report
        run: |
          echo "# Daily Edge Function Performance Report" > performance-summary.md
          echo "Generated: $(date)" >> performance-summary.md
          echo "" >> performance-summary.md
          
          for report in test-reports/*.json; do
            if [ -f "$report" ]; then
              scenario=$(jq -r '.config.edgeFunctionUrl' "$report" | grep -o '[^/]*$')
              success_rate=$(jq '.summary.successRate' "$report")
              avg_time=$(jq '.summary.averageResponseTime' "$report")
              p95_time=$(jq '.summary.p95ResponseTime' "$report")
              
              echo "## Scenario: $scenario" >> performance-summary.md
              echo "- Success Rate: ${success_rate}%" >> performance-summary.md
              echo "- Average Response Time: ${avg_time}ms" >> performance-summary.md
              echo "- P95 Response Time: ${p95_time}ms" >> performance-summary.md
              echo "" >> performance-summary.md
            fi
          done
      
      - name: Archive Performance Summary
        uses: actions/upload-artifact@v4
        with:
          name: daily-performance-summary
          path: performance-summary.md
          retention-days: 90