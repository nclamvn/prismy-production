# ===============================================
# Performance Monitoring Pipeline
# Automated performance testing and monitoring
# ===============================================

name: Performance Monitoring

on:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours
  push:
    branches: [main]
    paths:
      - 'app/**'
      - 'lib/**'
      - 'components/**'
      - 'pages/**'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to test'
        required: true
        default: 'production'
        type: choice
        options:
        - production
        - staging

env:
  NODE_VERSION: '20'
  PNPM_VERSION: '8'

jobs:
  # ===============================================
  # Lighthouse Performance Testing
  # ===============================================
  lighthouse:
    name: Lighthouse Performance
    runs-on: ubuntu-latest
    timeout-minutes: 20

    strategy:
      matrix:
        url:
          - '/'
          - '/dashboard'
          - '/translate'
          - '/pricing'
          - '/login'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Get environment URL
        id: get-url
        run: |
          if [[ "${{ github.event.inputs.environment }}" == "staging" ]]; then
            echo "base_url=https://staging.prismy.com" >> $GITHUB_OUTPUT
          else
            echo "base_url=https://prismy.com" >> $GITHUB_OUTPUT
          fi

      - name: Lighthouse CI
        uses: treosh/lighthouse-ci-action@v10
        with:
          urls: |
            ${{ steps.get-url.outputs.base_url }}${{ matrix.url }}
          configPath: './.lighthouserc.json'
          uploadArtifacts: true
          temporaryPublicStorage: true

      - name: Parse Lighthouse Results
        run: |
          # Extract performance metrics
          PERF_SCORE=$(jq -r '.audits.performance.score * 100' .lighthouseci/*/lhr-*.json | head -1)
          ACCESSIBILITY_SCORE=$(jq -r '.audits.accessibility.score * 100' .lighthouseci/*/lhr-*.json | head -1)
          BEST_PRACTICES_SCORE=$(jq -r '.audits["best-practices"].score * 100' .lighthouseci/*/lhr-*.json | head -1)
          SEO_SCORE=$(jq -r '.audits.seo.score * 100' .lighthouseci/*/lhr-*.json | head -1)
          
          FCP=$(jq -r '.audits["first-contentful-paint"].numericValue' .lighthouseci/*/lhr-*.json | head -1)
          LCP=$(jq -r '.audits["largest-contentful-paint"].numericValue' .lighthouseci/*/lhr-*.json | head -1)
          CLS=$(jq -r '.audits["cumulative-layout-shift"].numericValue' .lighthouseci/*/lhr-*.json | head -1)
          
          echo "URL: ${{ matrix.url }}"
          echo "Performance: ${PERF_SCORE}"
          echo "Accessibility: ${ACCESSIBILITY_SCORE}"
          echo "Best Practices: ${BEST_PRACTICES_SCORE}"
          echo "SEO: ${SEO_SCORE}"
          echo "FCP: ${FCP}ms"
          echo "LCP: ${LCP}ms"
          echo "CLS: ${CLS}"
          
          # Store metrics for aggregation
          cat > metrics.json << EOF
          {
            "url": "${{ matrix.url }}",
            "performance": ${PERF_SCORE},
            "accessibility": ${ACCESSIBILITY_SCORE},
            "best_practices": ${BEST_PRACTICES_SCORE},
            "seo": ${SEO_SCORE},
            "fcp": ${FCP},
            "lcp": ${LCP},
            "cls": ${CLS}
          }
          EOF

      - name: Upload metrics
        uses: actions/upload-artifact@v3
        with:
          name: lighthouse-metrics-${{ strategy.job-index }}
          path: metrics.json

  # ===============================================
  # Load Testing
  # ===============================================
  load-test:
    name: Load Testing
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'workflow_dispatch' || github.event_name == 'schedule'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install k6
        run: |
          curl https://github.com/grafana/k6/releases/download/v0.47.0/k6-v0.47.0-linux-amd64.tar.gz -L | tar xvz --strip-components 1

      - name: Get environment URL
        id: get-url
        run: |
          if [[ "${{ github.event.inputs.environment }}" == "staging" ]]; then
            echo "base_url=https://staging.prismy.com" >> $GITHUB_OUTPUT
          else
            echo "base_url=https://prismy.com" >> $GITHUB_OUTPUT
          fi

      - name: Create load test script
        run: |
          cat > load-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate } from 'k6/metrics';

          const errorRate = new Rate('errors');
          const BASE_URL = __ENV.BASE_URL || 'https://prismy.com';

          export const options = {
            stages: [
              { duration: '2m', target: 10 },   // Ramp up to 10 users
              { duration: '5m', target: 10 },   // Stay at 10 users
              { duration: '2m', target: 20 },   // Ramp up to 20 users
              { duration: '5m', target: 20 },   // Stay at 20 users
              { duration: '2m', target: 0 },    // Ramp down to 0 users
            ],
            thresholds: {
              http_req_duration: ['p(95)<2000'], // 95% of requests must complete within 2s
              http_req_failed: ['rate<0.05'],    // Error rate must be less than 5%
              errors: ['rate<0.05'],
            },
          };

          export default function () {
            const pages = [
              '/',
              '/api/health',
              '/pricing',
              '/login'
            ];

            for (const page of pages) {
              const response = http.get(`${BASE_URL}${page}`);
              
              const result = check(response, {
                'status is 200': (r) => r.status === 200,
                'response time < 2000ms': (r) => r.timings.duration < 2000,
              });
              
              errorRate.add(!result);
              
              sleep(1);
            }
          }
          EOF

      - name: Run load test
        run: |
          ./k6 run load-test.js --env BASE_URL=${{ steps.get-url.outputs.base_url }} --out json=load-test-results.json

      - name: Parse load test results
        run: |
          # Extract key metrics from load test
          AVG_RESPONSE_TIME=$(jq -r '.metrics.http_req_duration.values.avg' load-test-results.json)
          P95_RESPONSE_TIME=$(jq -r '.metrics.http_req_duration.values["p(95)"]' load-test-results.json)
          ERROR_RATE=$(jq -r '.metrics.http_req_failed.values.rate * 100' load-test-results.json)
          REQUESTS_PER_SEC=$(jq -r '.metrics.http_reqs.values.rate' load-test-results.json)
          
          echo "Average Response Time: ${AVG_RESPONSE_TIME}ms"
          echo "95th Percentile Response Time: ${P95_RESPONSE_TIME}ms"
          echo "Error Rate: ${ERROR_RATE}%"
          echo "Requests per Second: ${REQUESTS_PER_SEC}"
          
          # Check thresholds
          if (( $(echo "$P95_RESPONSE_TIME > 2000" | bc -l) )); then
            echo "❌ Performance threshold exceeded: P95 response time is ${P95_RESPONSE_TIME}ms"
            exit 1
          fi
          
          if (( $(echo "$ERROR_RATE > 5" | bc -l) )); then
            echo "❌ Error rate threshold exceeded: ${ERROR_RATE}%"
            exit 1
          fi
          
          echo "✅ Load test passed all thresholds"

      - name: Upload load test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: load-test-results
          path: load-test-results.json

  # ===============================================
  # API Performance Testing
  # ===============================================
  api-performance:
    name: API Performance
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install autocannon
        run: npm install -g autocannon

      - name: Get environment URL
        id: get-url
        run: |
          if [[ "${{ github.event.inputs.environment }}" == "staging" ]]; then
            echo "base_url=https://staging.prismy.com" >> $GITHUB_OUTPUT
          else
            echo "base_url=https://prismy.com" >> $GITHUB_OUTPUT
          fi

      - name: Test API endpoints
        run: |
          # Test critical API endpoints
          ENDPOINTS=(
            "/api/health"
            "/api/status" 
            "/api/metrics"
          )
          
          for endpoint in "${ENDPOINTS[@]}"; do
            echo "Testing ${endpoint}..."
            
            autocannon -c 10 -d 30 -j "${{ steps.get-url.outputs.base_url }}${endpoint}" > "results-${endpoint//\//-}.json"
            
            # Extract metrics
            AVG_LATENCY=$(jq -r '.latency.average' "results-${endpoint//\//-}.json")
            P99_LATENCY=$(jq -r '.latency.p99' "results-${endpoint//\//-}.json")
            REQUESTS_PER_SEC=$(jq -r '.requests.average' "results-${endpoint//\//-}.json")
            
            echo "${endpoint}: Avg latency ${AVG_LATENCY}ms, P99 ${P99_LATENCY}ms, ${REQUESTS_PER_SEC} req/s"
            
            # Check if latency is acceptable (< 500ms average)
            if (( $(echo "$AVG_LATENCY > 500" | bc -l) )); then
              echo "❌ High latency detected for ${endpoint}: ${AVG_LATENCY}ms"
            fi
          done

      - name: Upload API performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: api-performance-results
          path: results-*.json

  # ===============================================
  # Database Performance Monitoring
  # ===============================================
  database-performance:
    name: Database Performance
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: github.event_name == 'schedule'

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Check RDS Performance Insights
        run: |
          # Get RDS instance identifier
          DB_INSTANCE="production-prismy-db"
          
          # Get Performance Insights data
          END_TIME=$(date -u +%s)
          START_TIME=$((END_TIME - 3600))  # Last hour
          
          # Get top SQL statements by execution time
          aws pi get-resource-metrics \
            --service-type RDS \
            --identifier $DB_INSTANCE \
            --start-time $START_TIME \
            --end-time $END_TIME \
            --period-in-seconds 300 \
            --metric-queries '[
              {
                "Metric": "db.SQL.Execute.avg",
                "GroupBy": {"Group": "db.sql_tokenized.statement"}
              }
            ]' > db-performance.json
          
          # Check for slow queries (> 1 second)
          SLOW_QUERIES=$(jq -r '.MetricList[0].DataPoints[] | select(.Value > 1000) | .Value' db-performance.json | wc -l)
          
          if [ "$SLOW_QUERIES" -gt 0 ]; then
            echo "⚠️ Found $SLOW_QUERIES slow queries (>1s execution time)"
          else
            echo "✅ No slow queries detected"
          fi

      - name: Check database connections
        run: |
          # Get current connection count
          aws cloudwatch get-metric-statistics \
            --namespace AWS/RDS \
            --metric-name DatabaseConnections \
            --dimensions Name=DBInstanceIdentifier,Value=production-prismy-db \
            --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
            --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
            --period 300 \
            --statistics Average > db-connections.json
          
          # Check if connection count is high (>80% of max)
          MAX_CONNECTIONS=100  # Adjust based on your RDS instance
          AVG_CONNECTIONS=$(jq -r '.Datapoints | map(.Average) | add / length' db-connections.json)
          CONNECTION_PERCENTAGE=$(echo "scale=2; $AVG_CONNECTIONS * 100 / $MAX_CONNECTIONS" | bc)
          
          echo "Average connections: $AVG_CONNECTIONS ($CONNECTION_PERCENTAGE%)"
          
          if (( $(echo "$CONNECTION_PERCENTAGE > 80" | bc -l) )); then
            echo "⚠️ High database connection usage: $CONNECTION_PERCENTAGE%"
          fi

  # ===============================================
  # Performance Report Generation
  # ===============================================
  performance-report:
    name: Generate Performance Report
    runs-on: ubuntu-latest
    needs: [lighthouse, load-test, api-performance]
    if: always()
    timeout-minutes: 10

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v3

      - name: Aggregate performance metrics
        run: |
          # Combine all Lighthouse metrics
          echo "# Performance Report" > performance-report.md
          echo "" >> performance-report.md
          echo "**Generated:** $(date -u)" >> performance-report.md
          echo "**Environment:** ${{ github.event.inputs.environment || 'production' }}" >> performance-report.md
          echo "" >> performance-report.md
          
          echo "## Lighthouse Scores" >> performance-report.md
          echo "" >> performance-report.md
          echo "| Page | Performance | Accessibility | Best Practices | SEO | FCP | LCP |" >> performance-report.md
          echo "|------|-------------|---------------|----------------|-----|-----|-----|" >> performance-report.md
          
          # Process lighthouse metrics
          for file in lighthouse-metrics-*/metrics.json; do
            if [ -f "$file" ]; then
              URL=$(jq -r '.url' "$file")
              PERF=$(jq -r '.performance' "$file")
              A11Y=$(jq -r '.accessibility' "$file")
              BP=$(jq -r '.best_practices' "$file")
              SEO=$(jq -r '.seo' "$file")
              FCP=$(jq -r '.fcp' "$file")
              LCP=$(jq -r '.lcp' "$file")
              
              echo "| $URL | $PERF% | $A11Y% | $BP% | $SEO% | ${FCP}ms | ${LCP}ms |" >> performance-report.md
            fi
          done
          
          echo "" >> performance-report.md
          echo "## Load Testing Results" >> performance-report.md
          echo "" >> performance-report.md
          
          if [ -f "load-test-results/load-test-results.json" ]; then
            AVG_RT=$(jq -r '.metrics.http_req_duration.values.avg' load-test-results/load-test-results.json)
            P95_RT=$(jq -r '.metrics.http_req_duration.values["p(95)"]' load-test-results/load-test-results.json)
            ERROR_RATE=$(jq -r '.metrics.http_req_failed.values.rate * 100' load-test-results/load-test-results.json)
            RPS=$(jq -r '.metrics.http_reqs.values.rate' load-test-results/load-test-results.json)
            
            echo "- **Average Response Time:** ${AVG_RT}ms" >> performance-report.md
            echo "- **95th Percentile:** ${P95_RT}ms" >> performance-report.md
            echo "- **Error Rate:** ${ERROR_RATE}%" >> performance-report.md
            echo "- **Requests/Second:** ${RPS}" >> performance-report.md
          fi
          
          echo "" >> performance-report.md
          echo "## Recommendations" >> performance-report.md
          echo "" >> performance-report.md
          echo "- Monitor Core Web Vitals trends" >> performance-report.md
          echo "- Optimize images and static assets" >> performance-report.md
          echo "- Implement caching strategies" >> performance-report.md
          echo "- Review database query performance" >> performance-report.md

      - name: Upload performance report
        uses: actions/upload-artifact@v3
        with:
          name: performance-report
          path: performance-report.md

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            if (fs.existsSync('performance-report.md')) {
              const report = fs.readFileSync('performance-report.md', 'utf8');
              
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: `## 📊 Performance Test Results\n\n${report}`
              });
            }

      - name: Send Slack notification
        if: needs.load-test.result == 'failure' || needs.lighthouse.result == 'failure'
        uses: 8398a7/action-slack@v3
        with:
          status: 'failure'
          channel: '#performance'
          text: |
            🚨 Performance tests failed for ${{ github.repository }}
            Check the detailed report in the workflow artifacts.
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}